{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44d70279",
   "metadata": {},
   "source": [
    "## XCurve.AUROC.losses.StandardAUROC\n",
    "This chapter will introduce how to conduct AUROC-oriented training on multiclass long-tail CIFAR-10 dataset.\n",
    "Note that, in the following, we take SquareAUCLoss as an example. The others (i.e., HingeAUCLoss and ExpAUCLoss) stay similar to this one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11e29c3b",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "To begin with, we should import some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from XCurve.AUROC.dataloaders import get_datasets # dataset of Xcurve\n",
    "from XCurve.AUROC.dataloaders import get_data_loaders # dataloader of Xcurve\n",
    "from XCurve.AUROC.losses import SquareAUCLoss # loss of AUROC\n",
    "from torch.optim import SGD # optimier (or one can use any optimizer supported by PyTorch)\n",
    "from XCurve.AUROC.models import generate_net # create model or you can adopt any DNN models by Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f0b4319",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Set seed to ensure a reproducible experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809d23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1024\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24912924",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Formulate your model, criterion and optimizer\n",
    "\n",
    "### Model\n",
    "Our XCurve supports any pytorch-driven DNN model. In the XCurve, we implement ResNet-family backbone (18, 20, 35, etc.), Densenet121 and MLP.\n",
    "> XCurve.AUROC.models.resnet18(num_classes=1, pretrained=None)\n",
    "\n",
    "### Criterion\n",
    "To decouple the pairwisely interdependent AUC risk, we use an instance-wise reformulation strategy to obtain an end-to-end AUROC-oriented AT method.\n",
    "> XCurve.AUROC.losses.SquareAUCLoss(self, num_classes, gamma=1, transform='ovo')\n",
    "\n",
    "> XCurve.AUROC.losses.HingeAUCLoss(self, num_classes, gamma=1, transform='ovo')\n",
    "\n",
    "> XCurve.AUROC.losses.ExpAUCLoss(self, num_classes, gamma=1, transform='ovo')\n",
    "\n",
    "#### Parameters\n",
    "- num_classes (int): the number of classes in the classification problem. In this case, since we have more than two classes, num_classes is set to be greater than 2.\n",
    "- gamma (float): the safe margin for the loss. If the distance between a sample and the decision boundary of its true class is less than gamma, the sample is considered correctly classified (Default [1.0]).\n",
    "- transform (str): the manner of computing the multi-class AUROC Metric. In this case, it is set to \"ovo\", which stands for \"one-vs-one\". This means that all possible pairs of classes are compared, and the AUROC metric is computed for each pair separately. The final AUROC score is the average of all pairwise scores (Default ['ovo']).\n",
    "\n",
    "### Optimizer\n",
    "The optimizer variable is created using the SGD optimizer from the torch.optim module.\n",
    "\n",
    "#### Parameters\n",
    "- params: parameters in your pytorch model\n",
    "- lr: the learning rate for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2406eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params to create model\n",
    "args = edict({\n",
    "    \"model_type\": \"resnet18\", # (support resnet, densenet121 and mlp)\n",
    "    \"num_classes\": 10, # number of class\n",
    "    \"pretrained\": None # if the model is pretrained\n",
    "})\n",
    "# Or you can adopt any DNN models by Pytorch\n",
    "model = generate_net(args).cuda() # generate pytorch model \n",
    "\n",
    "num_classes = 10\n",
    "criterion = SquareAUCLoss(\n",
    "    num_classes=num_classes, # number of classes\n",
    "    gamma=1.0, # safe margin\n",
    "    transform=\"ovo\" # the manner of computing the multi-classes AUROC Metric ('ovo' or 'ova').\n",
    ") # create loss criterion\n",
    "optimizer = SGD(model.parameters(), lr=0.01) # create optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b59a874e",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Now, we can start adversarial training!\n",
    "\n",
    "### Dataset and DataLoaders\n",
    "Before training, you should load your customized datasets and dataloaders. \n",
    "The get_datasets() function is utilized to load the dataset and create train, validation, and test datasets. The get_data_loaders() function is then employed to create dataloaders for the train, validation, and test datasets.\n",
    "> XCurve.AUROC.dataloaders.get_datasets(dataset_args)\n",
    "\n",
    "> XCurve.AUROC.dataloaders.get_data_loaders(train_set, val_set, test_set, train_batch_sizeï¼Œ test_batch_size, num_workers=4, rpos = 1, rneg = 4)\n",
    "\n",
    "#### Parameters\n",
    "In the provided code, the dataset_args dictionary contains various parameters used to configure the dataset. Some of the crucial parameters in the dataset_args dictionary are explained below:\n",
    "\n",
    "- data_dir (str): the relative path of the dataset. \n",
    "- input_size (list): the size of the input images in pixels.\n",
    "- norm_params (EasyDict, list, float): the normalization parameters for the dataset. Specifically, it includes the mean and standard deviation values for the RGB channels of the images.\n",
    "- use_lmdb (bool): whether the dataset should be loaded as an LMDB database or not.\n",
    "- sampler (EasyDict, int): only employed for binary classification and contains the sampling rate for positive and negative examples in the dataset.\n",
    "- aug (bool): whether data augmentation should be used during training or not.\n",
    "- class2id (EasyDict, int): a dictionary mapping the class labels to their corresponding IDs. Here, the minority class has a label of 1 and all other classes have a label of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset params, see our doc. for more details.\n",
    "dataset_args = edict({\n",
    "    \"data_dir\": \"cifar-10-long-tail/\", # relative path of dataset\n",
    "    \"input_size\": [32, 32],\n",
    "    \"norm_params\": {\n",
    "        \"mean\": [123.675, 116.280, 103.530],\n",
    "        \"std\": [58.395, 57.120, 57.375]\n",
    "        },\n",
    "    \"use_lmdb\": True,\n",
    "    \"resampler_type\": \"None\",\n",
    "    \"npy_style\": True,\n",
    "    \"aug\": True, \n",
    "    \"num_classes\": num_classes\n",
    "})\n",
    "\n",
    "train_set, val_set, test_set = get_datasets(dataset_args) # load dataset\n",
    "trainloader, valloader, testloader = get_data_loaders(\n",
    "    train_set,\n",
    "    val_set,\n",
    "    test_set,\n",
    "    train_batch_size=32,\n",
    "    test_batch_size =64\n",
    ") # load dataloader\n",
    "# Note that, in the get_datasets(), we conduct stratified sampling for train_set  \n",
    "# using the StratifiedSampler at from XCurve.AUROC.dataloaders import StratifiedSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The provided code block delineates the forward pass of a neural network model for one epoch.\n",
    "\n",
    "- In line 1, a `for` loop is employed to iterate over the training data loader (`trainloader`). The `enumerate` function is utilized to iterate over the data loader with an index. For each iteration of the loop, a batch of input (`x`) and target (`target`) tensors are extracted from the `trainloader` and transferred to the GPU using the `cuda()` method.\n",
    "- In line 2, the shape of the `target` tensor is printed. This tensor has a shape of `[batch_size, ]`, where `batch_size` is the number of samples in the current batch. It is also noted that the model is anticipated to output predictions in the range of `[0, 1]` for binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "- In line 3, the input tensor `x` is passed through the model using the `model` object. The output of the model is a tensor of shape `[batch_size, num_classes]` if `num_classes > 2`, or `[batch_size, ]` otherwise. This output tensor is passed through a sigmoid function using the `torch.sigmoid` method to obtain a probability distribution over the classes.\n",
    "- In line 4, the binary cross-entropy loss is calculated between the predicted probabilities and the true targets using the `criterion` object.\n",
    "- In line 5, the current loss value is displayed on the console if the current iteration index is a multiple of 30.\n",
    "- In line 6, the gradients of the loss concerning the model parameters are computed using the `backward` method.\n",
    "- In line 7, the optimizer's gradients are set to zero employing the `zero_grad` method.\n",
    "- In line 8, the optimizer's step function is invoked using the `step` method to update the model parameters based on the computed gradients.\n",
    "\n",
    "It is worth noting that the backward pass and optimizer step functions are employed to update the model's parameters and enhance its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5070081353187561\n",
      "loss: 0.5045551061630249\n"
     ]
    }
   ],
   "source": [
    "# forward of model for one epoch\n",
    "for index, (x, target) in enumerate(trainloader):\n",
    "    x, target  = x.cuda(), target.cuda()\n",
    "    # target.shape => [batch_size, ]\n",
    "    # Note that we ask for the prediction of the model among [0,1] \n",
    "    # for any binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "    \n",
    "    # forward\n",
    "    pred = torch.sigmoid(model(x)) # [batch_size, num_classess] when num_classes > 2, o.w. output [batch_size, ] \n",
    "    loss = criterion(pred, target)\n",
    "    if index % 30 == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39837c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
