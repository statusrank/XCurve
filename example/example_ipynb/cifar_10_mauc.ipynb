{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCurve Library\n",
    "\n",
    "This code imports several libraries and modules.\n",
    "\n",
    "### Import optimizer, loss function, dataset, and dataloader.\n",
    "\n",
    "The provided code block imports the necessary packages and modules for using the SquareAUCLoss loss function and the SGD optimizer in a PyTorch neural network implementation.\n",
    "\n",
    "#### Importing Required Packages\n",
    "1. The `torch` package is imported, which is the primary package used for building and training neural networks in PyTorch.\n",
    "2. The `EasyDict` module from the `easydict` package is imported and aliased as `edict`. This module provides a simple way to create and access dictionaries with dot notation, making it easier to read and write code.\n",
    "\n",
    "#### Importing SquareAUCLoss Function\n",
    "1. The `SquareAUCLoss` class is imported from the `XCurve.AUROC.losses` module.\n",
    "2. This loss function is used to compute the area under the ROC curve (AUROC) in a binary classification problem, which is a common evaluation metric used in machine learning.\n",
    "3. The `SquareAUCLoss` function takes in the predicted scores and true labels as inputs and computes the loss based on the square of the difference between the predicted scores and the true labels.\n",
    "4. This loss function is a variation of the AUCLoss function that has been shown to improve the performance of binary classification models.\n",
    "\n",
    "#### Importing SGD Optimizer\n",
    "1. The `SGD` optimizer is imported from the `torch.optim` module.\n",
    "2. This optimizer is a stochastic gradient descent optimizer that is commonly used for training deep neural networks.\n",
    "3. The `SGD` optimizer takes in the model parameters and the learning rate as inputs and updates the parameters based on the gradients computed during the backpropagation process.\n",
    "4. Other optimizers like `Adam` or `Adagrad` can also be used depending on the specific needs of the model or task.\n",
    "\n",
    "Overall, the imported packages and modules are essential for implementing and training a neural network in PyTorch. The SquareAUCLoss loss function is a specialized loss function that can be used to optimize the performance of binary classification models based on the AUROC metric. The SGD optimizer is a commonly used optimization algorithm that updates the model parameters based on the gradients computed during the backpropagation process. By importing these modules, the user can easily use these functions in their neural network implementation and fine-tune the model to achieve optimal performance for their specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# import loss of AUROC\n",
    "from XCurve.AUROC.losses import SquareAUCLoss\n",
    "\n",
    "# import optimier (or one can use any optimizer supported by PyTorch)\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters, create the model.\n",
    "\n",
    "In the provided code, a deep learning model is created using the `generate_net` function from the `XCurve.AUROC.models` module. This function generates a neural network model of a specified type (e.g., resnet18, resnet20, densenet121, or mlp).\n",
    "\n",
    "The `args` variable is an `edict`, which is a dictionary-like object that allows attribute-style access (e.g., `args.model_type` instead of `args['model_type']`). It contains the parameters for creating the model. Some of the essential parameters are explained below:\n",
    "\n",
    "- `model_type`: This parameter specifies the type of model to be generated. In this case, it is set to `\"resnet18\"`, which means that a ResNet-18 model will be generated.\n",
    "\n",
    "- `num_classes`: This parameter specifies the number of classes in the classification problem. In this case, it is set to `10`.\n",
    "\n",
    "- `pretrained`: This parameter specifies whether the model should be loaded with pre-trained weights. In this case, it is set to `None`, which means that the model will not be pre-trained.\n",
    "\n",
    "The `generate_net` function returns a PyTorch model object, stored in the `model` variable. The model is then moved to the GPU using the `cuda` method.\n",
    "\n",
    "It is worth noting that the `generate_net` function is part of the XCurve package, which is a Python package for computing the Area Under the Receiver Operating Characteristic Curve (AUROC) and other performance metrics for binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model or you can adopt any DNN models by Pytorch\n",
    "from XCurve.AUROC.models import generate_net\n",
    "\n",
    "# set params to create model\n",
    "args = edict({\n",
    "    \"model_type\": \"resnet18\", # (support resnet18,resnet20, densenet121 and mlp)\n",
    "    \"num_classes\": 10, # number of class\n",
    "    \"pretrained\": None # if the model is pretrained\n",
    "})\n",
    "model = generate_net(args).cuda() # generate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimizer and loss function\n",
    "\n",
    "The provided code block shows the creation of a loss criterion and an optimizer for a deep neural network (DNN) model with 2 classes using the PAUCLoss loss function and the SGD optimizer algorithm.\n",
    "\n",
    "## Initializing num_classes Variable\n",
    "1. The `num_classes` variable is initialized to 10, which indicates that the multi-classification problem is being solved.\n",
    "2. This variable is used to specify the number of classes in the dataset, which is important for defining the network architecture and selecting the appropriate loss function.\n",
    "\n",
    "## Creating Optimizer\n",
    "1. The `SGD` optimizer is created using the `model.parameters()` method, which returns an iterator over the model's trainable parameters.\n",
    "2. The learning rate of the optimizer is set to 0.01, which determines the step size of the parameter updates during training.\n",
    "3. Other hyperparameters like momentum and weight decay can also be specified as arguments to the optimizer constructor.\n",
    "\n",
    "## Creating Loss Criterion\n",
    "1. The `SquareAUCLoss` loss criterion is created using the specified parameters:\n",
    "    a. `num_classes` is set to the previously initialized value of 2.\n",
    "    b. `gamma` is set to 1.0, which is the safe margin used in the loss function to penalize false positives and false negatives.\n",
    "    c. `transform` is set to \"ovo\", which stands for \"one-vs-one\" and indicates that the multi-class AUROC metric will be computed using pairwise comparisons between classes.\n",
    "2. The `SquareAUCLoss` loss criterion is a specialized loss function that is used to optimize the performance of binary classification models based on the AUROC metric.\n",
    "3. The specified parameters are used to customize the behavior of the loss function, such as the way the multi-class AUROC metric is computed.\n",
    "\n",
    "Overall, the `num_classes`, optimizer, and loss criterion are important components of a PyTorch neural network implementation. The `num_classes` variable is used to specify the number of classes in the dataset, which is important for defining the network architecture and selecting the appropriate loss function. The optimizer is responsible for updating the model parameters during training, and the `SGD` optimizer is commonly used for training deep neural networks. The `SquareAUCLoss` loss criterion is a specialized loss function that is used to optimize the performance of binary classification models based on the AUROC metric. By customizing the parameters of the loss function and the optimizer, the user can fine-tune the model to achieve optimal performance for their specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "optimizer = SGD(model.parameters(), lr=0.01) # create optimizer\n",
    "\n",
    "# create loss criterion\n",
    "criterion = SquareAUCLoss(\n",
    "    num_classes=num_classes, # number of classes\n",
    "    gamma=1.0, # safe margin\n",
    "    transform=\"ovo\" # the manner of computing the multi-classes AUROC Metric ('ovo' or 'ova').\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset (train_set, val_set, test_set) and dataloader (trainloader)\n",
    "\n",
    "In the provided code, the `dataset_args` dictionary contains various parameters used to configure the dataset. Some of the crucial parameters in the `dataset_args` dictionary are explained below:\n",
    "\n",
    "- `data_dir`: This parameter specifies the relative path of the dataset. In this case, the dataset is stored in the `cifar-10-long-tail/` directory.\n",
    "\n",
    "- `input_size`: This parameter specifies the size of the input images in pixels. Here, the input images have a dimension of 32x32 pixels.\n",
    "\n",
    "- `norm_params`: This parameter contains the normalization parameters for the dataset. Specifically, it includes the mean and standard deviation values for the RGB channels of the images.\n",
    "\n",
    "- `use_lmdb`: This parameter specifies whether the dataset should be loaded as an LMDB database or not.\n",
    "\n",
    "- `sampler`: This parameter is only employed for binary classification and contains the sampling rate for positive and negative examples in the dataset.\n",
    "\n",
    "- `aug`: This parameter specifies whether data augmentation should be used during training or not.\n",
    "\n",
    "- `class2id`: This parameter is a dictionary mapping the class labels to their corresponding IDs. Here, the minority class has a label of `1` and all other classes have a label of `0`.\n",
    "\n",
    "The `get_datasets()` function is utilized to load the dataset and create train, validation, and test datasets. The `get_data_loaders()` function is then employed to create dataloaders for the train, validation, and test datasets.\n",
    "\n",
    "It is worth noting that the `get_datasets()` function utilizes stratified sampling for the train set. Specifically, it employs the `StratifiedSampler` from the `XCurve.AUROC.dataloaders` module to ensure that the number of samples from each class is balanced in the train set.\n",
    "\n",
    "For more information on the `StratifiedSampler`, please refer to the official PyTorch documentation [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.StratifiedSampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feng\\Xcurve\\XCurve\\example\\data\\XCurve\\AUROC\\dataloaders\\sampler.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append(neg_samples, ignore_index=False)\n"
     ]
    }
   ],
   "source": [
    "# set dataset params, see our doc. for more details.\n",
    "dataset_args = edict({\n",
    "    \"data_dir\": \"cifar-10-long-tail/\", # relative path of dataset\n",
    "    \"input_size\": [32, 32],\n",
    "    \"norm_params\": {\n",
    "        \"mean\": [123.675, 116.280, 103.530],\n",
    "        \"std\": [58.395, 57.120, 57.375]\n",
    "        },\n",
    "    \"use_lmdb\": True,\n",
    "    \"resampler_type\": \"None\",\n",
    "    \"sampler\": { # only used for binary classification\n",
    "        \"rpos\": 1,\n",
    "        \"rneg\": 10\n",
    "        },\n",
    "    \"npy_style\": True,\n",
    "    \"aug\": True, \n",
    "    \"class2id\": { # positive (minority) class idx\n",
    "        \"1\": 1, \"0\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0,\n",
    "        \"6\":0, \"7\":0, \"8\":0, \"9\":0\n",
    "    }\n",
    "})\n",
    "\n",
    "train_set, val_set, test_set = get_datasets(dataset_args) # load dataset\n",
    "trainloader, valloader, testloader = get_data_loaders(\n",
    "    train_set,\n",
    "    val_set,\n",
    "    test_set,\n",
    "    train_batch_size=32,\n",
    "    test_batch_size =64\n",
    ") # load dataloader\n",
    "# Note that, in the get_datasets(), we conduct stratified sampling for train_set  \n",
    "# using the StratifiedSampler at from XCurve.AUROC.dataloaders import StratifiedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The provided code block delineates the forward pass of a neural network model for one epoch.\n",
    "\n",
    "- In line 1, a `for` loop is employed to iterate over the training data loader (`trainloader`). The `enumerate` function is utilized to iterate over the data loader with an index. For each iteration of the loop, a batch of input (`x`) and target (`target`) tensors are extracted from the `trainloader` and transferred to the GPU using the `cuda()` method.\n",
    "\n",
    "- In line 2, the shape of the `target` tensor is printed. This tensor has a shape of `[batch_size, ]`, where `batch_size` is the number of samples in the current batch. It is also noted that the model is anticipated to output predictions in the range of `[0, 1]` for binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "\n",
    "- In line 3, the input tensor `x` is passed through the model using the `model` object. The output of the model is a tensor of shape `[batch_size, num_classes]` if `num_classes > 2`, or `[batch_size, ]` otherwise. This output tensor is passed through a sigmoid function using the `torch.sigmoid` method to obtain a probability distribution over the classes.\n",
    "\n",
    "- In line 4, the binary cross-entropy loss is calculated between the predicted probabilities and the true targets using the `criterion` object.\n",
    "\n",
    "- In line 5, the current loss value is displayed on the console if the current iteration index is a multiple of 30.\n",
    "\n",
    "- In line 6, the gradients of the loss concerning the model parameters are computed using the `backward` method.\n",
    "\n",
    "- In line 7, the optimizer's gradients are set to zero employing the `zero_grad` method.\n",
    "\n",
    "- In line 8, the optimizer's step function is invoked using the `step` method to update the model parameters based on the computed gradients.\n",
    "\n",
    "It is worth noting that the backward pass and optimizer step functions are employed to update the model's parameters and enhance its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.15990693867206573\n",
      "loss: 0.2334757000207901\n",
      "loss: 0.30239760875701904\n",
      "loss: 0.1381421536207199\n",
      "loss: 0.13158641755580902\n",
      "loss: 0.38331863284111023\n",
      "loss: 0.08875473588705063\n",
      "loss: 0.20242173969745636\n",
      "loss: 0.32185712456703186\n",
      "loss: 0.15229110419750214\n",
      "loss: 0.11186783015727997\n",
      "loss: 0.11620910465717316\n"
     ]
    }
   ],
   "source": [
    "# forward of model for one epoch\n",
    "for index, (x, target) in enumerate(trainloader):\n",
    "    x, target  = x.cuda(), target.cuda()\n",
    "    # target.shape => [batch_size, ]\n",
    "    # Note that we ask for the prediction of the model among [0,1] \n",
    "    # for any binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "    \n",
    "    # forward\n",
    "    pred = torch.sigmoid(model(x)) # [batch_size, num_classess] when num_classes > 2, o.w. output [batch_size, ] \n",
    "    loss = criterion(pred, target)\n",
    "    if index % 30 == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
