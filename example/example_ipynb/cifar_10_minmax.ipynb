{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCurve Library\n",
    "\n",
    "This code imports several libraries and modules.\n",
    "\n",
    "### Import optimizer, loss function, dataset, and dataloader.\n",
    "\n",
    "#### Importing Torch and EasyDict\n",
    "1. The `torch` module is imported, which is the primary library used for building and training deep learning models in PyTorch.\n",
    "2. The `EasyDict` class from the `easydict` module is imported and renamed as `edict`. This is a convenience class that allows dictionary keys to be accessed as attributes.\n",
    "\n",
    "#### Importing AUROC Losses\n",
    "1. The `get_losses` function is imported from the `XCurve.AUROC.losses` module.\n",
    "2. This function returns a dictionary of pre-defined PyTorch loss functions that are commonly used to optimize the AUROC metric.\n",
    "3. The available loss functions include `BCEWithLogitsLoss`, `FocalLoss`, and `SquareAUCLoss`, among others.\n",
    "\n",
    "#### Importing Optimizer\n",
    "1. The `SGD4MinMaxPAUC` optimizer is imported from the `XCurve.AUROC.optimizer` module.\n",
    "2. This optimizer is specifically designed for optimizing the MinMaxPAUC metric, which is a modified version of the AUROC metric that is better suited for imbalanced datasets.\n",
    "3. The `SGD` optimizer can also be used instead of `SGD4MinMaxPAUC`, or any other optimizer supported by PyTorch.\n",
    "\n",
    "Overall, this code block imports necessary PyTorch and XCurve.AUROC modules for a neural network implementation. The `torch` module is a fundamental library for building and training deep learning models. The `EasyDict` class from the `easydict` module is used to simplify dictionary access by allowing keys to be accessed as attributes. The `get_losses` function from the `XCurve.AUROC.losses` module provides a convenient way to import pre-defined loss functions that are commonly used to optimize the AUROC metric. The `SGD4MinMaxPAUC` optimizer from the `XCurve.AUROC.optimizer` module is a specialized optimizer designed for optimizing the MinMaxPAUC metric, which is a modified version of the AUROC metric that is more suitable for imbalanced datasets. By using these imported modules, the user can easily fine-tune their neural network implementation to achieve optimal performance for their specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# import loss of AUROC\n",
    "from XCurve.AUROC.losses import get_losses\n",
    "\n",
    "# import optimier (or one can use any optimizer supported by PyTorch)\n",
    "from XCurve.AUROC.optimizer import SGD4MinMaxPAUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters, create the model.\n",
    "\n",
    "In the provided code, a deep learning model is created using the `generate_net` function from the `XCurve.AUROC.models` module. This function generates a neural network model of a specified type (e.g., resnet18, resnet20, densenet121, or mlp).\n",
    "\n",
    "The `args` variable is an `edict`, which is a dictionary-like object that allows attribute-style access (e.g., `args.model_type` instead of `args['model_type']`). It contains the parameters for creating the model. Some of the essential parameters are explained below:\n",
    "\n",
    "- `model_type`: This parameter specifies the type of model to be generated. In this case, it is set to `\"resnet18\"`, which means that a ResNet-18 model will be generated.\n",
    "\n",
    "- `num_classes`: This parameter specifies the number of classes in the classification problem. In this case, it is set to `2`.\n",
    "\n",
    "- `pretrained`: This parameter specifies whether the model should be loaded with pre-trained weights. In this case, it is set to `None`, which means that the model will not be pre-trained.\n",
    "\n",
    "The `generate_net` function returns a PyTorch model object, stored in the `model` variable. The model is then moved to the GPU using the `cuda` method.\n",
    "\n",
    "It is worth noting that the `generate_net` function is part of the XCurve package, which is a Python package for computing the Area Under the Receiver Operating Characteristic Curve (AUROC) and other performance metrics for binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model or you can adopt any DNN models by Pytorch\n",
    "from XCurve.AUROC.models import generate_net\n",
    "\n",
    "# set params to create model\n",
    "args = edict({\n",
    "    \"model_type\": \"resnet18\", # (support resnet18,resnet20, densenet121 and mlp)\n",
    "    \"num_classes\": 2, # number of class\n",
    "    \"pretrained\": None # if the model is pretrained\n",
    "})\n",
    "model = generate_net(args).cuda() # generate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimizer and loss function\n",
    "\n",
    "\n",
    "The provided code block defines the `num_classes` variable and an `args_training` object that contains various training-related parameters, including loss type, learning rate, and weight decay. It also initializes two tensors, `a` and `b`, with requires_grad set to True, and defines an optimizer using the `SGD4MinMaxPAUC` class and a loss criterion using the `get_losses` function.\n",
    "\n",
    "#### Defining num_classes\n",
    "1. The `num_classes` variable is defined as having a value of 2.\n",
    "2. This variable is likely used to indicate the number of classes in a classification task.\n",
    "\n",
    "#### Defining Training Parameters\n",
    "1. The `args_training` object is defined using the `edict` class from the `easydict` module.\n",
    "2. This object contains various training-related parameters, including batch sizes, number of workers, loss type, loss parameters, learning rate, weight decay, momentum, and more.\n",
    "3. The `train_batch_size` and `test_batch_size` parameters indicate the number of samples per batch for training and testing, respectively.\n",
    "4. The `num_workers` parameter specifies the number of subprocesses to use for data loading.\n",
    "5. The `loss_type` parameter specifies the type of loss function to use during training and can be set to \"MinMaxPAUC\" or any other supported loss function.\n",
    "6. The `loss_params` parameter is a dictionary that contains specific parameters for the selected loss function, which in this case is `MinMaxPAUC`. These parameters include the number of classes, gamma value, E_k value, weight scheme, reduction type, AUC type, first state loss function, epsilon value, and regularization values.\n",
    "7. The `lr`, `weight_decay`, `momentum`, `nesterov`, `lr_decay_rate`, `lr_decay_epochs`, and `epoch_num` parameters all relate to the optimization algorithm used for training, including the learning rate, weight decay, momentum, and number of epochs to train for.\n",
    "8. The `metric_params` parameter is a dictionary that specifies the alpha and beta values for the MinMaxPAUC metric.\n",
    "9. The `save_path` parameter specifies the directory to save the trained model to.\n",
    "10. The `seed` parameter specifies the random seed to use for reproducibility.\n",
    "\n",
    "#### Initializing Tensors\n",
    "1. Two tensors, `a` and `b`, are initialized with a value of 0.5 and `requires_grad=True`.\n",
    "2. These tensors are likely used as variables that need to be optimized during training.\n",
    "\n",
    "#### Defining Optimizer\n",
    "1. An optimizer is defined using the `SGD4MinMaxPAUC` class from the `XCurve.AUROC.optimizer` module.\n",
    "2. The optimizer is initialized with the model parameters, the `a` and `b` tensors, the learning rate, weight decay, clip value, and epoch to optimize.\n",
    "3. This optimizer is specifically designed for optimizing the MinMaxPAUC metric, which is a modified version of the AUROC metric that is better suited for imbalanced datasets.\n",
    "\n",
    "#### Defining Loss Criterion\n",
    "1. A loss criterion is defined using the `get_losses` function from the `XCurve.AUROC.losses` module.\n",
    "2. The `args_training` object is passed as an argument to the `get_losses` function, which returns the appropriate loss function based on the specified loss type and parameters.\n",
    "\n",
    "Overall, this code block defines various training-related parameters using the `args_training` object and initializes the `SGD4MinMaxPAUC` optimizer and loss criterion using appropriate modules from the XCurve.AUROC library. The `num_classes` variable is used to indicate the number of classes in a classification task. Additionally, two tensors, `a` and `b`, are initialized with a value of 0.5 and `requires_grad=True`, and are likely used as variables that need to be optimized during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "args_training = edict({\n",
    "    \"train_batch_size\": 32,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "    \"loss_type\": \"MinMaxPAUC\",\n",
    "    \"loss_params\": {\n",
    "        \"num_classes\": 2,\n",
    "        \"gamma\": 1.0,\n",
    "        \"E_k\": 3,\n",
    "        \"weight_scheme\": \"Poly\",\n",
    "        \"reduction\": \"mean\",\n",
    "        \"AUC_type\": \"OP\",\n",
    "        \"first_state_loss\": torch.nn.BCELoss(),\n",
    "        \"eps\": 1e-6,\n",
    "        \"reg_a\":0.1,\n",
    "        \"reg_b\":0.2\n",
    "    },\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"momentum\": 0.9,\n",
    "    \"nesterov\": True,\n",
    "    \"lr_decay_rate\": 0.99,\n",
    "    \"lr_decay_epochs\": 1,\n",
    "    \"epoch_num\": 50,\n",
    "    \"metric_params\": {\n",
    "        \"alpha\": 0.4,\n",
    "        \"beta\": 0.1\n",
    "    },\n",
    "    \"save_path\": \"./save/\",\n",
    "    \"seed\": 7\n",
    "})\n",
    "\n",
    "a, b = torch.tensor(0.5, requires_grad=True), torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "optimizer = SGD4MinMaxPAUC(\n",
    "    params=model.parameters(),\n",
    "    a=a,\n",
    "    b=b,\n",
    "    lr=args_training ['lr'],\n",
    "    weight_decay=args_training['weight_decay'],\n",
    "    clip_value=5,\n",
    "    epoch_to_opt=1\n",
    ")\n",
    "\n",
    "# create loss criterion\n",
    "criterion = get_losses(args_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset (train_set, val_set, test_set) and dataloader (trainloader)\n",
    "\n",
    "In the provided code, the `dataset_args` dictionary contains various parameters used to configure the dataset. Some of the crucial parameters in the `dataset_args` dictionary are explained below:\n",
    "\n",
    "- `data_dir`: This parameter specifies the relative path of the dataset. In this case, the dataset is stored in the `cifar-10-long-tail/` directory.\n",
    "\n",
    "- `input_size`: This parameter specifies the size of the input images in pixels. Here, the input images have a dimension of 32x32 pixels.\n",
    "\n",
    "- `norm_params`: This parameter contains the normalization parameters for the dataset. Specifically, it includes the mean and standard deviation values for the RGB channels of the images.\n",
    "\n",
    "- `use_lmdb`: This parameter specifies whether the dataset should be loaded as an LMDB database or not.\n",
    "\n",
    "- `sampler`: This parameter is only employed for binary classification and contains the sampling rate for positive and negative examples in the dataset.\n",
    "\n",
    "- `aug`: This parameter specifies whether data augmentation should be used during training or not.\n",
    "\n",
    "- `class2id`: This parameter is a dictionary mapping the class labels to their corresponding IDs. Here, the minority class has a label of `1` and all other classes have a label of `0`.\n",
    "\n",
    "The `get_datasets()` function is utilized to load the dataset and create train, validation, and test datasets. The `get_data_loaders()` function is then employed to create dataloaders for the train, validation, and test datasets.\n",
    "\n",
    "It is worth noting that the `get_datasets()` function utilizes stratified sampling for the train set. Specifically, it employs the `StratifiedSampler` from the `XCurve.AUROC.dataloaders` module to ensure that the number of samples from each class is balanced in the train set.\n",
    "\n",
    "For more information on the `StratifiedSampler`, please refer to the official PyTorch documentation [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.StratifiedSampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feng\\Xcurve\\XCurve\\example\\data\\XCurve\\AUROC\\dataloaders\\sampler.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append(neg_samples, ignore_index=False)\n"
     ]
    }
   ],
   "source": [
    "# set dataset params, see our doc. for more details.\n",
    "dataset_args = edict({\n",
    "    \"data_dir\": \"cifar-10-long-tail/\", # relative path of dataset\n",
    "    \"input_size\": [32, 32],\n",
    "    \"norm_params\": {\n",
    "        \"mean\": [123.675, 116.280, 103.530],\n",
    "        \"std\": [58.395, 57.120, 57.375]\n",
    "        },\n",
    "    \"use_lmdb\": True,\n",
    "    \"resampler_type\": \"None\",\n",
    "    \"sampler\": { # only used for binary classification\n",
    "        \"rpos\": 1,\n",
    "        \"rneg\": 10\n",
    "        },\n",
    "    \"npy_style\": True,\n",
    "    \"aug\": True, \n",
    "    \"class2id\": { # positive (minority) class idx\n",
    "        \"1\": 1, \"0\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0,\n",
    "        \"6\":0, \"7\":0, \"8\":0, \"9\":0\n",
    "    }\n",
    "})\n",
    "\n",
    "train_set, val_set, test_set = get_datasets(dataset_args) # load dataset\n",
    "trainloader, valloader, testloader = get_data_loaders(\n",
    "    train_set,\n",
    "    val_set,\n",
    "    test_set,\n",
    "    train_batch_size=32,\n",
    "    test_batch_size =64\n",
    ") # load dataloader\n",
    "# Note that, in the get_datasets(), we conduct stratified sampling for train_set  \n",
    "# using the StratifiedSampler at from XCurve.AUROC.dataloaders import StratifiedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The provided code block delineates the forward pass of a neural network model for one epoch.\n",
    "\n",
    "- In line 1, a `for` loop is employed to iterate over the training data loader (`trainloader`). The `enumerate` function is utilized to iterate over the data loader with an index. For each iteration of the loop, a batch of input (`x`) and target (`target`) tensors are extracted from the `trainloader` and transferred to the GPU using the `cuda()` method.\n",
    "\n",
    "- In line 2, the shape of the `target` tensor is printed. This tensor has a shape of `[batch_size, ]`, where `batch_size` is the number of samples in the current batch. It is also noted that the model is anticipated to output predictions in the range of `[0, 1]` for binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "\n",
    "- In line 3, the input tensor `x` is passed through the model using the `model` object. The output of the model is a tensor of shape `[batch_size, num_classes]` if `num_classes > 2`, or `[batch_size, ]` otherwise. This output tensor is passed through a sigmoid function using the `torch.sigmoid` method to obtain a probability distribution over the classes.\n",
    "\n",
    "- In line 4, the binary cross-entropy loss is calculated between the predicted probabilities and the true targets using the `criterion` object.\n",
    "\n",
    "- In line 5, the current loss value is displayed on the console if the current iteration index is a multiple of 30.\n",
    "\n",
    "- In line 6, the gradients of the loss concerning the model parameters are computed using the `backward` method.\n",
    "\n",
    "- In line 7, the optimizer's gradients are set to zero employing the `zero_grad` method.\n",
    "\n",
    "- In line 8, the optimizer's step function is invoked using the `step` method to update the model parameters based on the computed gradients.\n",
    "\n",
    "It is worth noting that the backward pass and optimizer step functions are employed to update the model's parameters and enhance its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.15990693867206573\n",
      "loss: 0.2334757000207901\n",
      "loss: 0.30239760875701904\n",
      "loss: 0.1381421536207199\n",
      "loss: 0.13158641755580902\n",
      "loss: 0.38331863284111023\n",
      "loss: 0.08875473588705063\n",
      "loss: 0.20242173969745636\n",
      "loss: 0.32185712456703186\n",
      "loss: 0.15229110419750214\n",
      "loss: 0.11186783015727997\n",
      "loss: 0.11620910465717316\n"
     ]
    }
   ],
   "source": [
    "# forward of model for one epoch\n",
    "for index, (x, target) in enumerate(trainloader):\n",
    "    x, target  = x.cuda(), target.cuda()\n",
    "    # target.shape => [batch_size, ]\n",
    "    # Note that we ask for the prediction of the model among [0,1] \n",
    "    # for any binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "    \n",
    "    # forward\n",
    "    pred = torch.sigmoid(model(x)) # [batch_size, num_classess] when num_classes > 2, o.w. output [batch_size, ] \n",
    "    loss = criterion(pred, target)\n",
    "    if index % 30 == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
