{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCurve Library\n",
    "\n",
    "This code imports several libraries and modules.\n",
    "\n",
    "### Import optimizer, loss function, dataset, and dataloader.\n",
    "\n",
    "#### Import Statements\n",
    "1. The `torch` module is imported, which is a popular open-source machine learning library used for building and training neural networks.\n",
    "2. The `easydict` module is imported and aliased as `edict`. This lightweight module provides a simple way to create dictionaries with attribute-style access.\n",
    "\n",
    "#### AUROC Loss Function\n",
    "1. The `PAUCLoss` class is imported from the `XCurve.AUROC.losses` module.\n",
    "2. This class implements a loss function for computing the probability AUROC (PAUC) of binary classification models given a set of predicted probabilities and true binary labels.\n",
    "3. The PAUC loss function has been shown to be effective in improving the performance of binary classification models.\n",
    "\n",
    "#### Optimizer Algorithm\n",
    "1. The SGD (Stochastic Gradient Descent) optimizer is imported from the `torch.optim` module.\n",
    "2. This optimizer is an implementation of stochastic gradient descent, a popular optimization algorithm for training deep neural networks.\n",
    "3. Other optimizers supported by PyTorch can also be used in place of SGD.\n",
    "\n",
    "It is worth noting that deep learning for binary classification tasks is a common practice in machine learning, and the use of specialized loss functions and optimizer algorithms can help improve the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# import loss of AUROC\n",
    "from XCurve.AUROC.losses import PAUCLoss\n",
    "\n",
    "# import optimier (or one can use any optimizer supported by PyTorch)\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the parameters, create the model.\n",
    "\n",
    "In the provided code, a deep learning model is created using the `generate_net` function from the `XCurve.AUROC.models` module. This function generates a neural network model of a specified type (e.g., resnet18, resnet20, densenet121, or mlp).\n",
    "\n",
    "The `args` variable is an `edict`, which is a dictionary-like object that allows attribute-style access (e.g., `args.model_type` instead of `args['model_type']`). It contains the parameters for creating the model. Some of the essential parameters are explained below:\n",
    "\n",
    "- `model_type`: This parameter specifies the type of model to be generated. In this case, it is set to `\"resnet18\"`, which means that a ResNet-18 model will be generated.\n",
    "\n",
    "- `num_classes`: This parameter specifies the number of classes in the classification problem. In this case, it is set to `2`.\n",
    "\n",
    "- `pretrained`: This parameter specifies whether the model should be loaded with pre-trained weights. In this case, it is set to `None`, which means that the model will not be pre-trained.\n",
    "\n",
    "The `generate_net` function returns a PyTorch model object, stored in the `model` variable. The model is then moved to the GPU using the `cuda` method.\n",
    "\n",
    "It is worth noting that the `generate_net` function is part of the XCurve package, which is a Python package for computing the Area Under the Receiver Operating Characteristic Curve (AUROC) and other performance metrics for binary and multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model or you can adopt any DNN models by Pytorch\n",
    "from XCurve.AUROC.models import generate_net\n",
    "\n",
    "# set params to create model\n",
    "args = edict({\n",
    "    \"model_type\": \"resnet18\", # (support resnet18,resnet20, densenet121 and mlp)\n",
    "    \"num_classes\": 2, # number of class\n",
    "    \"pretrained\": None # if the model is pretrained\n",
    "})\n",
    "model = generate_net(args).cuda() # generate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimizer and loss function\n",
    "\n",
    "The provided code block shows the creation of a loss criterion and an optimizer for a deep neural network (DNN) model with 2 classes using the PAUCLoss loss function and the SGD optimizer algorithm.\n",
    "\n",
    "#### Number of Classes\n",
    "1. The `num_classes` variable is initialized to 2, which indicates that there are two classes in the binary classification problem. \n",
    "2. This variable will be used later to specify the number of classes in the PAUCLoss function.\n",
    "\n",
    "#### SGD Optimizer\n",
    "1. The `SGD` optimizer is created with the `model.parameters()` as the input, which means that the optimizer will update the weights of the model during the training process.\n",
    "2. The learning rate for the optimizer is set as 0.01 through the `lr` parameter.\n",
    "3. The `optimizer` variable now contains an instance of the SGD optimizer that can be used to train the DNN model.\n",
    "\n",
    "#### PAUCLoss Loss Criterion\n",
    "1. The `PAUCLoss` class is instantiated and assigned to the `criterion` variable.\n",
    "2. The `gamma` parameter specifies the safe margin for the PAUCLoss function.\n",
    "3. The `E_k` parameter sets the number of warm-up epochs where the loss is computed using the `torch.nn.BCELoss()` function.\n",
    "4. The `weight_scheme` parameter specifies the weight scheme used in the calculation of the PAUCLoss function.\n",
    "5. The `num_classes` parameter is set to 2, which specifies the number of classes in the binary classification problem.\n",
    "6. The `reduction` parameter specifies the method of aggregating the loss.\n",
    "7. The `AUC_type` parameter specifies the type of optimization to be performed (OPAUC or TPAUC).\n",
    "8. The `first_state_loss` parameter specifies the loss function to be used during the warm-up epochs.\n",
    "9. The `eps` parameter is used to avoid zero gradient.\n",
    "\n",
    "Overall, the creation of the loss criterion and the optimizer are crucial steps in training a deep neural network model. The SGD optimizer is a commonly used optimization algorithm in deep learning, and the PAUCLoss function is a specialized loss function that has been shown to improve the performance of binary classification models. The parameters of the PAUCLoss function can be fine-tuned to optimize the performance of the model for specific tasks. For more information on the PAUCLoss function, please refer to the original paper [here](https://ieeexplore.ieee.org/document/8415427)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "optimizer = SGD(model.parameters(), lr=0.01) # create optimizer\n",
    "\n",
    "# create loss criterion\n",
    "criterion = PAUCLoss(\n",
    "    gamma=1, # safe margin\n",
    "    E_k=0, # warm-up epoch.\n",
    "    weight_scheme=\"Poly\", # weight scheme\n",
    "    num_classes=2, # number of classes\n",
    "    reduction=\"mean\", # loss aggregated manne\n",
    "    AUC_type=\"OP\", # (OPAUC or TPAUC optimization).\n",
    "    first_state_loss=torch.nn.BCELoss(), # warm-up loss\n",
    "    eps=1e-6 # avoid zero gradient\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset (train_set, val_set, test_set) and dataloader (trainloader)\n",
    "\n",
    "In the provided code, the `dataset_args` dictionary contains various parameters used to configure the dataset. Some of the crucial parameters in the `dataset_args` dictionary are explained below:\n",
    "\n",
    "- `data_dir`: This parameter specifies the relative path of the dataset. In this case, the dataset is stored in the `cifar-10-long-tail/` directory.\n",
    "\n",
    "- `input_size`: This parameter specifies the size of the input images in pixels. Here, the input images have a dimension of 32x32 pixels.\n",
    "\n",
    "- `norm_params`: This parameter contains the normalization parameters for the dataset. Specifically, it includes the mean and standard deviation values for the RGB channels of the images.\n",
    "\n",
    "- `use_lmdb`: This parameter specifies whether the dataset should be loaded as an LMDB database or not.\n",
    "\n",
    "- `sampler`: This parameter is only employed for binary classification and contains the sampling rate for positive and negative examples in the dataset.\n",
    "\n",
    "- `aug`: This parameter specifies whether data augmentation should be used during training or not.\n",
    "\n",
    "- `class2id`: This parameter is a dictionary mapping the class labels to their corresponding IDs. Here, the minority class has a label of `1` and all other classes have a label of `0`.\n",
    "\n",
    "The `get_datasets()` function is utilized to load the dataset and create train, validation, and test datasets. The `get_data_loaders()` function is then employed to create dataloaders for the train, validation, and test datasets.\n",
    "\n",
    "It is worth noting that the `get_datasets()` function utilizes stratified sampling for the train set. Specifically, it employs the `StratifiedSampler` from the `XCurve.AUROC.dataloaders` module to ensure that the number of samples from each class is balanced in the train set.\n",
    "\n",
    "For more information on the `StratifiedSampler`, please refer to the official PyTorch documentation [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.StratifiedSampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feng\\Xcurve\\XCurve\\example\\data\\XCurve\\AUROC\\dataloaders\\sampler.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.data = self.data.append(neg_samples, ignore_index=False)\n"
     ]
    }
   ],
   "source": [
    "# set dataset params, see our doc. for more details.\n",
    "dataset_args = edict({\n",
    "    \"data_dir\": \"cifar-10-long-tail/\", # relative path of dataset\n",
    "    \"input_size\": [32, 32],\n",
    "    \"norm_params\": {\n",
    "        \"mean\": [123.675, 116.280, 103.530],\n",
    "        \"std\": [58.395, 57.120, 57.375]\n",
    "        },\n",
    "    \"use_lmdb\": True,\n",
    "    \"resampler_type\": \"None\",\n",
    "    \"sampler\": { # only used for binary classification\n",
    "        \"rpos\": 1,\n",
    "        \"rneg\": 10\n",
    "        },\n",
    "    \"npy_style\": True,\n",
    "    \"aug\": True, \n",
    "    \"class2id\": { # positive (minority) class idx\n",
    "        \"1\": 1, \"0\":0, \"2\":0, \"3\":0, \"4\":0, \"5\":0,\n",
    "        \"6\":0, \"7\":0, \"8\":0, \"9\":0\n",
    "    }\n",
    "})\n",
    "\n",
    "train_set, val_set, test_set = get_datasets(dataset_args) # load dataset\n",
    "trainloader, valloader, testloader = get_data_loaders(\n",
    "    train_set,\n",
    "    val_set,\n",
    "    test_set,\n",
    "    train_batch_size=32,\n",
    "    test_batch_size =64\n",
    ") # load dataloader\n",
    "# Note that, in the get_datasets(), we conduct stratified sampling for train_set  \n",
    "# using the StratifiedSampler at from XCurve.AUROC.dataloaders import StratifiedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "The provided code block delineates the forward pass of a neural network model for one epoch.\n",
    "\n",
    "- In line 1, a `for` loop is employed to iterate over the training data loader (`trainloader`). The `enumerate` function is utilized to iterate over the data loader with an index. For each iteration of the loop, a batch of input (`x`) and target (`target`) tensors are extracted from the `trainloader` and transferred to the GPU using the `cuda()` method.\n",
    "\n",
    "- In line 2, the shape of the `target` tensor is printed. This tensor has a shape of `[batch_size, ]`, where `batch_size` is the number of samples in the current batch. It is also noted that the model is anticipated to output predictions in the range of `[0, 1]` for binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "\n",
    "- In line 3, the input tensor `x` is passed through the model using the `model` object. The output of the model is a tensor of shape `[batch_size, num_classes]` if `num_classes > 2`, or `[batch_size, ]` otherwise. This output tensor is passed through a sigmoid function using the `torch.sigmoid` method to obtain a probability distribution over the classes.\n",
    "\n",
    "- In line 4, the binary cross-entropy loss is calculated between the predicted probabilities and the true targets using the `criterion` object.\n",
    "\n",
    "- In line 5, the current loss value is displayed on the console if the current iteration index is a multiple of 30.\n",
    "\n",
    "- In line 6, the gradients of the loss concerning the model parameters are computed using the `backward` method.\n",
    "\n",
    "- In line 7, the optimizer's gradients are set to zero employing the `zero_grad` method.\n",
    "\n",
    "- In line 8, the optimizer's step function is invoked using the `step` method to update the model parameters based on the computed gradients.\n",
    "\n",
    "It is worth noting that the backward pass and optimizer step functions are employed to update the model's parameters and enhance its performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.15990693867206573\n",
      "loss: 0.2334757000207901\n",
      "loss: 0.30239760875701904\n",
      "loss: 0.1381421536207199\n",
      "loss: 0.13158641755580902\n",
      "loss: 0.38331863284111023\n",
      "loss: 0.08875473588705063\n",
      "loss: 0.20242173969745636\n",
      "loss: 0.32185712456703186\n",
      "loss: 0.15229110419750214\n",
      "loss: 0.11186783015727997\n",
      "loss: 0.11620910465717316\n"
     ]
    }
   ],
   "source": [
    "# forward of model for one epoch\n",
    "for index, (x, target) in enumerate(trainloader):\n",
    "    x, target  = x.cuda(), target.cuda()\n",
    "    # target.shape => [batch_size, ]\n",
    "    # Note that we ask for the prediction of the model among [0,1] \n",
    "    # for any binary (i.e., sigmoid) or multi-class (i.e., softmax) AUROC optimization.\n",
    "    \n",
    "    # forward\n",
    "    pred = torch.sigmoid(model(x)) # [batch_size, num_classess] when num_classes > 2, o.w. output [batch_size, ] \n",
    "    loss = criterion(pred, target)\n",
    "    if index % 30 == 0:\n",
    "        print(\"loss:\", loss.item())\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
