{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k Accuracy\n",
    "> XCurve.Metrics.TopkAcc(y_pred, y_true, k_list=(1, ))\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the top-k accuracy for each k in the specified k-list. For each k value, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{Acc}_k(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I} \\left( f(x_i)_{y_i} > f(x_i)_{[k]} \\right),\n",
    "$$\n",
    "where $f(x_i)_{y_i}$ is the score of the ground-truth label, $f(x_i)_{[k]}$ is the $k$-largest value in $f(x) \\in \\mathbb{R}^C$, $C$ denotes the number of classes, and $\\mathbb{I}(\\cdot)$ is the indicator function. \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> Optimizing Partial Area Under the Top-k Curve: Theory and Practice. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. T-PAMI, 2023. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- y_pred: Prediction score (torch array with shape (n_samples, n_classes)).\n",
    "- y_true: True labels (torch array with shape (n_samples,)). \n",
    "- k_list: The specified k-list\n",
    "\n",
    "#### Return\n",
    "- Top-k Accuracy (list): return the top-k accuracy for each k in the specified k-list, where the values are multiplied by 100.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 4 2 2 3 0 4 3 3] [[0.33260182 0.98972819 0.47387256 0.06931025 0.40067703]\n",
      " [0.70520369 0.92568264 0.65599723 0.48662344 0.92633993]\n",
      " [0.76879399 0.48764938 0.89407772 0.5022212  0.69910168]\n",
      " [0.78757722 0.43742444 0.05884364 0.15259803 0.28110998]\n",
      " [0.3643331  0.11342609 0.82531249 0.22478265 0.95019877]\n",
      " [0.63188258 0.09784013 0.43409818 0.40966188 0.27887016]\n",
      " [0.38734299 0.90546    0.4032867  0.31872577 0.65433305]\n",
      " [0.29183237 0.13497379 0.61450353 0.80428317 0.33457867]\n",
      " [0.45970975 0.83844255 0.62131009 0.82368393 0.39325256]\n",
      " [0.10044784 0.42764775 0.72443996 0.14037189 0.49142701]]\n",
      "[tensor(0.), tensor(60.)]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import TopkAcc\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# binary cases\n",
    "n_samples, C = 10, 5\n",
    "k_list = (1, 3)\n",
    "y_true = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "y_pred = np.random.rand(n_samples, C)\n",
    "print(y_true, y_pred)\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "\n",
    "topk_acc=TopkAcc(y_pred, y_true, k_list)\n",
    "print(topk_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTKC\n",
    "> XCurve.Metrics.AUTKC(y_pred, y_true, k_list=(1, ))\n",
    "\n",
    "### Brief Introductions\n",
    "Compute AUTKC for each K in the specified K-list. For each K value, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{AUTKC}(f) = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{I} \\left( f(x_i)_{y_i} > f(x_i)_{[k]} \\right),\n",
    "$$\n",
    "where $f(x_i)_{y_i}$ is the score of the ground-truth label, $f(x_i)_{[k]}$ is the $k$-largest value in $f(x) \\in \\mathbb{R}^C$, $C$ denotes the number of classes, and $\\mathbb{I}(\\cdot)$ is the indicator function. \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> Optimizing Partial Area Under the Top-k Curve: Theory and Practice. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. T-PAMI, 2023. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- y_pred: Prediction score (torch array with shape (n_samples, n_classes)).\n",
    "- y_true: True labels (torch array with shape (n_samples,)). \n",
    "- k_list: The specified k-list\n",
    "\n",
    "#### Return\n",
    "- AUTKC (list): return the AUTKC value for each K in the specified K-list, where the values are multiplied by 100.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 3 0 2 2 0 0 2 3] [[0.12004942 0.12839059 0.09746845 0.10007967 0.5003975 ]\n",
      " [0.29630375 0.98446441 0.71087458 0.95552689 0.1764935 ]\n",
      " [0.20025918 0.06045881 0.86476045 0.56423726 0.20515452]\n",
      " [0.25393672 0.45903589 0.85093299 0.08130737 0.08847788]\n",
      " [0.13481334 0.57613637 0.10768599 0.6509431  0.16144303]\n",
      " [0.68972733 0.89358    0.80235074 0.76609356 0.45606972]\n",
      " [0.99336448 0.80887329 0.51614641 0.74440291 0.01400317]\n",
      " [0.05791392 0.70465184 0.21733082 0.20240532 0.17013353]\n",
      " [0.45001052 0.14551943 0.63749125 0.65415112 0.99344252]\n",
      " [0.38893428 0.62077235 0.72965816 0.10166493 0.91797621]]\n",
      "[tensor(10.), tensor(36.6667)]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import AUTKC\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# binary cases\n",
    "n_samples, C = 10, 5\n",
    "k_list = (1, 3)\n",
    "y_true = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "y_pred = np.random.rand(n_samples, C)\n",
    "print(y_true, y_pred)\n",
    "y_true = torch.LongTensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "\n",
    "autkc=AUTKC(y_pred, y_true, k_list)\n",
    "print(autkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Open-Set F-score\n",
    "> XCurve.Metrics.MacroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute Macro Open-Set F-score for each threshold in the given decision-making conditions. For each threshold, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{F-score}(f) = 2 \\times \\frac{\\text{P}_\\text{k}(f) \\times \\text{R}_\\text{k}(f)}{\\text{P}_\\text{k}(f) + \\text{R}_\\text{k}(f)},\n",
    "$$\n",
    "where the precision and recall of known classes is defined as:\n",
    "$$\n",
    "    \\text{P}_\\text{k}(f) = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i}, \\text{R}_\\text{k}(f) = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i},\n",
    "$$\n",
    "and $\\text{TP}_i, \\text{FP}_i, \\text{FN}_i$ are the True Positive (TP), False Positive (FP), False Negative (FN) performance of the close-set class $i$ under the given open-set decision threshold $t$, respectively; $C$ denotes the number of known classes (close-set). \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "> Nearest neighbors distance ratio open-set classifier. Pedro Ribeiro Mendes Júnior, Roberto Medeiros de Souza, Rafael de Oliveira Werneck, Bernardo V. Stein, Daniel V. Pazinato, Waldir R. de Almeida, Otávio A. B. Penatti, Ricardo da Silva Torres, Anderson Rocha. Mach. Lean., 2017.\n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- close_pred: Predicted close-set scores (numpy array with shape (n_samples, n_classes)).\n",
    "- close_labels: True close-set label (numpy array with shape (n_samples,)). \n",
    "- open_pred: Predicted open-set score (numpy array with shape (n_samples,)). \n",
    "- open_labels: True open-set label, which is binary (numpy array with shape (n_samples,)). \n",
    "- t_list: the list of open-set decision threshold\n",
    "\n",
    "#### Return\n",
    "- Macro Open-Set F-score (list): return the Macro Open-Set F-score under the given threshold list.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43207594 0.36688422 0.63439254 0.19906785 0.26619923]\n",
      " [0.41970256 0.31710435 0.88533964 0.01593951 0.86327503]\n",
      " [0.73055808 0.97621639 0.92143159 0.87420198 0.74084541]\n",
      " [0.45749194 0.70147553 0.95055184 0.62927756 0.21682865]\n",
      " [0.07494416 0.49015044 0.18860223 0.41202115 0.38479732]\n",
      " [0.94415413 0.52529156 0.87318235 0.67849652 0.42992943]\n",
      " [0.29741235 0.6768609  0.42534278 0.37351204 0.05564604]\n",
      " [0.61385606 0.15822404 0.13756377 0.04797314 0.46250745]\n",
      " [0.30735434 0.43712791 0.13427006 0.29507074 0.29014883]\n",
      " [0.77635792 0.41135596 0.18913517 0.9740344  0.57632315]] [4 4 3 1 4 0 4 2 4 4] [0.98221357 0.07159885 0.4740611  0.94801461 0.90819031 0.15446603\n",
      " 0.55011334 0.45546207 0.38194535 0.85267424] [0 1 0 0 1 0 0 1 0 0]\n",
      "0.2 [0, 0.11764705882352941, 0.2, 0.15384615384615383, 0.15384615384615383]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import MacroF\n",
    "import numpy as np \n",
    "\n",
    "# binary cases\n",
    "n_samples, C = 10, 5\n",
    "t_list = (0,2, 0.4, 0.6, 0.8)\n",
    "close_pred = np.random.rand(n_samples, C)\n",
    "close_labels = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "open_pred = np.random.rand(n_samples, )\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(close_pred, close_labels, open_pred, open_labels)\n",
    "\n",
    "MacroF_score = MacroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "print(max(MacroF_score), MacroF_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro Open-Set F-score\n",
    "> XCurve.Metrics.MicroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute Micro Open-Set F-score for each threshold in the given decision-making conditions. For each threshold, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{F-score}(f) = 2 \\times \\frac{\\text{P}_\\text{k}(f) \\times \\text{R}_\\text{k}(f)}{\\text{P}_\\text{k}(f) + \\text{R}_\\text{k}(f)},\n",
    "$$\n",
    "where the precision and recall of known classes is defined as:\n",
    "$$\n",
    "    \\text{P}_\\text{k}(f) = \\frac{\\sum_{i=1}^{C} \\text{TP}_i}{\\sum_{i=1}^{C} (\\text{TP}_i + \\text{FP}_i)}, \\text{R}_\\text{k}(f) = \\frac{\\sum_{i=1}^{C} \\text{TP}_i}{\\sum_{i=1}^{C} (\\text{TP}_i + \\text{FN}_i)},\n",
    "$$\n",
    "and $\\text{TP}_i, \\text{FP}_i, \\text{FN}_i$ are the True Positive (TP), False Positive (FP), False Negative (FN) performance of the close-set class $i$ under the given open-set decision threshold $t$, respectively; $C$ denotes the number of known classes (close-set). \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "> Nearest neighbors distance ratio open-set classifier. Pedro Ribeiro Mendes Júnior, Roberto Medeiros de Souza, Rafael de Oliveira Werneck, Bernardo V. Stein, Daniel V. Pazinato, Waldir R. de Almeida, Otávio A. B. Penatti, Ricardo da Silva Torres, Anderson Rocha. Mach. Lean., 2017.\n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- close_pred: Predicted close-set scores (numpy array with shape (n_samples, n_classes)).\n",
    "- close_labels: True close-set label (numpy array with shape (n_samples,)). \n",
    "- open_pred: Predicted open-set score (numpy array with shape (n_samples,)). \n",
    "- open_labels: True open-set label, which is binary (numpy array with shape (n_samples,)). \n",
    "- t_list: the list of open-set decision threshold\n",
    "\n",
    "#### Return\n",
    "- Micro Open-Set F-score (list): return the Micro Open-Set F-score under the given threshold list.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50532347 0.75768319 0.0338213  0.49375395 0.10316429]\n",
      " [0.47783392 0.98188272 0.60321685 0.10631093 0.62648642]\n",
      " [0.94702207 0.1355103  0.25133449 0.86555639 0.90573772]\n",
      " [0.73986343 0.79257048 0.31293413 0.49847066 0.39805277]\n",
      " [0.38348539 0.92483025 0.61505449 0.06673217 0.41664348]\n",
      " [0.04830171 0.6963944  0.40276529 0.25889272 0.83648787]\n",
      " [0.01283603 0.24802913 0.09742623 0.16305417 0.61959027]\n",
      " [0.23533863 0.92013827 0.28045214 0.86397515 0.77963527]\n",
      " [0.76734916 0.69051219 0.75284054 0.21671037 0.64503991]\n",
      " [0.18437406 0.59378276 0.42091092 0.1484393  0.19760516]] [0 1 3 0 2 4 0 1 2 3] [0.66937573 0.16086238 0.33551949 0.47830559 0.71937674 0.20951835\n",
      " 0.94355376 0.83535767 0.5842739  0.97289716] [1 0 0 0 1 0 0 0 1 0]\n",
      "0.34285714285714286 [0, 0.23529411764705882, 0.34285714285714286, 0.3, 0.2727272727272727]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import MicroF\n",
    "import numpy as np \n",
    "\n",
    "# binary cases\n",
    "n_samples, C = 10, 5\n",
    "t_list = (0,2, 0.4, 0.6, 0.8)\n",
    "close_pred = np.random.rand(n_samples, C)\n",
    "close_labels = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "open_pred = np.random.rand(n_samples, )\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(close_pred, close_labels, open_pred, open_labels)\n",
    "\n",
    "MicroF_score = MicroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "print(max(MicroF_score), MicroF_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
