{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k Accuracy\n",
    "> XCurve.Metrics.TopkAcc(y_pred, y_true, k_list=(1, ))\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the top-k accuracy for each k in the specified k-list. For each k value, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{Acc}_k(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I} \\left( f(x_i)_{y_i} > f(x_i)_{[k]} \\right),\n",
    "$$\n",
    "where $f(x_i)_{y_i}$ is the score of the ground-truth label, $f(x_i)_{[k]}$ is the $k$-largest value in $f(x) \\in \\mathbb{R}^C$, $C$ denotes the number of classes, and $\\mathbb{I}(\\cdot)$ is the indicator function. \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> Optimizing Partial Area Under the Top-k Curve: Theory and Practice. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. T-PAMI, 2023. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- y_pred: Prediction score (torch array with shape (n_samples, n_classes)).\n",
    "- y_true: True labels (torch array with shape (n_samples,)). \n",
    "- k_list: The specified k-list\n",
    "\n",
    "#### Return\n",
    "- Top-k Accuracy (list): return the top-k accuracy for each k in the specified k-list, where the values are multiplied by 100.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 2 2 4 0 2 4 1] [[0.41000211 0.97214757 0.65509522 0.39969803 0.04997318]\n",
      " [0.09357317 0.21498948 0.81335791 0.87925863 0.40536448]\n",
      " [0.8094639  0.81137964 0.83791497 0.45675286 0.83135504]\n",
      " [0.69556472 0.47180059 0.41107981 0.13141551 0.31448496]\n",
      " [0.30194047 0.30191417 0.57698903 0.55483532 0.67755728]\n",
      " [0.35768109 0.77799893 0.13355911 0.65852028 0.00349953]\n",
      " [0.97117831 0.891758   0.39642328 0.11143798 0.53984416]\n",
      " [0.51902947 0.21066057 0.28065688 0.11112596 0.54105724]\n",
      " [0.6238824  0.18503816 0.76485138 0.28166645 0.17688807]\n",
      " [0.60007232 0.48460844 0.92038521 0.77252945 0.52424144]]\n",
      "[tensor(10.), tensor(60.)]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import TopkAcc\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "n_samples, C = 10, 5\n",
    "k_list = (1, 3)\n",
    "y_true = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "y_pred = np.random.rand(n_samples, C)\n",
    "print(y_true, y_pred)\n",
    "y_true = torch.tensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "\n",
    "topk_acc=TopkAcc(y_pred, y_true, k_list)\n",
    "print(topk_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTKC\n",
    "> XCurve.Metrics.AUTKC(y_pred, y_true, k_list=(1, ))\n",
    "\n",
    "### Brief Introductions\n",
    "Compute AUTKC for each K in the specified K-list. For each K value, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{AUTKC}(f) = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{I} \\left( f(x_i)_{y_i} > f(x_i)_{[k]} \\right),\n",
    "$$\n",
    "where $f(x_i)_{y_i}$ is the score of the ground-truth label, $f(x_i)_{[k]}$ is the $k$-largest value in $f(x) \\in \\mathbb{R}^C$, $C$ denotes the number of classes, and $\\mathbb{I}(\\cdot)$ is the indicator function. \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> Optimizing Partial Area Under the Top-k Curve: Theory and Practice. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. T-PAMI, 2023. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- y_pred: Prediction score (torch array with shape (n_samples, n_classes)).\n",
    "- y_true: True labels (torch array with shape (n_samples,)). \n",
    "- k_list: The specified k-list\n",
    "\n",
    "#### Return\n",
    "- AUTKC (list): return the AUTKC value for each K in the specified K-list, where the values are multiplied by 100.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 0 4 4 2 4 0 4] [[0.79770745 0.15326916 0.91891386 0.48173586 0.2410106 ]\n",
      " [0.08958015 0.32332878 0.55872758 0.02814188 0.24945842]\n",
      " [0.26755323 0.65975121 0.99672491 0.47972119 0.93222953]\n",
      " [0.33323473 0.42421473 0.48545877 0.76385752 0.36521359]\n",
      " [0.2030766  0.72229981 0.9760547  0.98226655 0.96962791]\n",
      " [0.80669952 0.92862169 0.3876694  0.02891112 0.88540212]\n",
      " [0.31139694 0.39889434 0.78583398 0.68806438 0.38491996]\n",
      " [0.78025577 0.71777353 0.79466621 0.35252127 0.05149285]\n",
      " [0.26719754 0.04278924 0.79376542 0.41265465 0.02340955]\n",
      " [0.23926127 0.48539015 0.69163381 0.93408291 0.25859157]]\n",
      "[tensor(10.), tensor(23.3333)]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import AUTKC\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "n_samples, C = 10, 5\n",
    "k_list = (1, 3)\n",
    "y_true = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "y_pred = np.random.rand(n_samples, C)\n",
    "print(y_true, y_pred)\n",
    "y_true = torch.LongTensor(y_true)\n",
    "y_pred = torch.tensor(y_pred)\n",
    "\n",
    "autkc=AUTKC(y_pred, y_true, k_list)\n",
    "print(autkc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro Open-Set F-score\n",
    "> XCurve.Metrics.MacroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute Macro Open-Set F-score for each threshold in the given decision-making conditions. For each threshold, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{F-score}(f) = 2 \\times \\frac{\\text{P}_\\text{k}(f) \\times \\text{R}_\\text{k}(f)}{\\text{P}_\\text{k}(f) + \\text{R}_\\text{k}(f)},\n",
    "$$\n",
    "where the precision and recall of known classes is defined as:\n",
    "$$\n",
    "    \\text{P}_\\text{k}(f) = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i}, \\text{R}_\\text{k}(f) = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i},\n",
    "$$\n",
    "and $\\text{TP}_i, \\text{FP}_i, \\text{FN}_i$ are the True Positive (TP), False Positive (FP), False Negative (FN) performance of the close-set class $i$ under the given open-set decision threshold $t$, respectively; $C$ denotes the number of known classes (close-set). \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "\n",
    "> Nearest neighbors distance ratio open-set classifier. Pedro Ribeiro Mendes Júnior, Roberto Medeiros de Souza, Rafael de Oliveira Werneck, Bernardo V. Stein, Daniel V. Pazinato, Waldir R. de Almeida, Otávio A. B. Penatti, Ricardo da Silva Torres, Anderson Rocha. Mach. Lean., 2017.\n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- close_pred: Predicted close-set scores (numpy array with shape (n_samples, n_classes)).\n",
    "- close_labels: True close-set label (numpy array with shape (n_samples,)). \n",
    "- open_pred: Predicted open-set score (numpy array with shape (n_samples,)). \n",
    "- open_labels: True open-set label, which is binary (numpy array with shape (n_samples,)). \n",
    "- t_list: the list of open-set decision threshold\n",
    "\n",
    "#### Return\n",
    "- Macro Open-Set F-score (list): return the Macro Open-Set F-score under the given threshold list.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90564722 0.8053263  0.17874825 0.57933406 0.11532109]\n",
      " [0.48034266 0.61194355 0.94719088 0.55660287 0.58853507]\n",
      " [0.4758892  0.90484206 0.75893862 0.39644268 0.45118841]\n",
      " [0.67549929 0.87651129 0.43866011 0.99043902 0.60197289]\n",
      " [0.92537796 0.2890145  0.41369828 0.44596663 0.21948944]\n",
      " [0.45756761 0.41056895 0.36245375 0.26820873 0.40676823]\n",
      " [0.03324828 0.34954504 0.10089037 0.75705712 0.59347551]\n",
      " [0.73848811 0.93146745 0.67096405 0.42640257 0.6860138 ]\n",
      " [0.93382077 0.89890708 0.73958776 0.45686431 0.96612153]\n",
      " [0.87791676 0.90324339 0.82205357 0.00952616 0.53503324]] [4 0 3 1 1 0 4 3 3 4] [0.8672531  0.20129282 0.66544377 0.90079371 0.07603918 0.50352847\n",
      " 0.4520697  0.63229962 0.25474392 0.57482608] [0 1 1 1 0 0 0 1 0 0]\n",
      "0.16666666666666666 [0, 0.125, 0, 0.16666666666666666, 0.14285714285714288]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import MacroF\n",
    "import numpy as np \n",
    "\n",
    "n_samples, C = 10, 5\n",
    "t_list = (0,2, 0.4, 0.6, 0.8)\n",
    "close_pred = np.random.rand(n_samples, C)\n",
    "close_labels = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "open_pred = np.random.rand(n_samples, )\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(close_pred, close_labels, open_pred, open_labels)\n",
    "\n",
    "MacroF_score = MacroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "print(max(MacroF_score), MacroF_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro Open-Set F-score\n",
    "> XCurve.Metrics.MicroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute Micro Open-Set F-score for each threshold in the given decision-making conditions. For each threshold, this metric can be denoted as:\n",
    "$$\n",
    "    \\text{F-score}(f) = 2 \\times \\frac{\\text{P}_\\text{k}(f) \\times \\text{R}_\\text{k}(f)}{\\text{P}_\\text{k}(f) + \\text{R}_\\text{k}(f)},\n",
    "$$\n",
    "where the precision and recall of known classes is defined as:\n",
    "$$\n",
    "    \\text{P}_\\text{k}(f) = \\frac{\\sum_{i=1}^{C} \\text{TP}_i}{\\sum_{i=1}^{C} (\\text{TP}_i + \\text{FP}_i)}, \\text{R}_\\text{k}(f) = \\frac{\\sum_{i=1}^{C} \\text{TP}_i}{\\sum_{i=1}^{C} (\\text{TP}_i + \\text{FN}_i)},\n",
    "$$\n",
    "and $\\text{TP}_i, \\text{FP}_i, \\text{FN}_i$ are the True Positive (TP), False Positive (FP), False Negative (FN) performance of the close-set class $i$ under the given open-set decision threshold $t$, respectively; $C$ denotes the number of known classes (close-set). \n",
    "\n",
    "For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "\n",
    "> Nearest neighbors distance ratio open-set classifier. Pedro Ribeiro Mendes Júnior, Roberto Medeiros de Souza, Rafael de Oliveira Werneck, Bernardo V. Stein, Daniel V. Pazinato, Waldir R. de Almeida, Otávio A. B. Penatti, Ricardo da Silva Torres, Anderson Rocha. Mach. Lean., 2017.\n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- close_pred: Predicted close-set scores (numpy array with shape (n_samples, n_classes)).\n",
    "- close_labels: True close-set label (numpy array with shape (n_samples,)). \n",
    "- open_pred: Predicted open-set score (numpy array with shape (n_samples,)). \n",
    "- open_labels: True open-set label, which is binary (numpy array with shape (n_samples,)). \n",
    "- t_list: the list of open-set decision threshold\n",
    "\n",
    "#### Return\n",
    "- Micro Open-Set F-score (list): return the Micro Open-Set F-score under the given threshold list.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56298094 0.79601196 0.57447775 0.88358885 0.80678316]\n",
      " [0.96847627 0.42264948 0.14736004 0.43854344 0.43011639]\n",
      " [0.11712986 0.73055729 0.26893188 0.85416994 0.51656085]\n",
      " [0.41597478 0.39814307 0.27132573 0.92184759 0.79221889]\n",
      " [0.27722003 0.49416242 0.38642151 0.71236756 0.72753309]\n",
      " [0.72463737 0.90163486 0.07130694 0.50976338 0.37445718]\n",
      " [0.85101195 0.82753815 0.83338266 0.62708768 0.17609595]\n",
      " [0.7476741  0.7629192  0.61110947 0.06186983 0.87875607]\n",
      " [0.07032869 0.63660758 0.69293559 0.6077748  0.19170085]\n",
      " [0.25840685 0.49028485 0.10347185 0.79955499 0.2974612 ]] [3 2 0 2 3 1 2 4 4 0] [0.9489464  0.13229303 0.65540627 0.17362515 0.0477727  0.62379578\n",
      " 0.16948579 0.67225321 0.60475531 0.41842617] [1 1 0 1 1 0 0 0 0 1]\n",
      "0.3 [0, 0.3, 0, 0, 0.3]\n"
     ]
    }
   ],
   "source": [
    "from Metrics import MicroF\n",
    "import numpy as np \n",
    "\n",
    "n_samples, C = 10, 5\n",
    "t_list = (0,2, 0.4, 0.6, 0.8)\n",
    "close_pred = np.random.rand(n_samples, C)\n",
    "close_labels = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "open_pred = np.random.rand(n_samples, )\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(close_pred, close_labels, open_pred, open_labels)\n",
    "\n",
    "MicroF_score = MicroF(close_pred, close_labels, open_pred, open_labels, t_list)\n",
    "print(max(MicroF_score), MicroF_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close-set Accuracy\n",
    "> XCurve.Metrics.ClosedSetAcc(preds, labels)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the close-set accuracy, which is exactly the same as that for the traditional multiclass classification. For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- preds: Predicted close-set scores (numpy array with shape (n_samples, n_classes)).\n",
    "- labels: True close-set label (numpy array with shape (n_samples,)). \n",
    "\n",
    "#### Return\n",
    "- Close-set accuracy (int): return the close-set accuracy of the given predictions.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.96802685e-01 4.49498566e-01 3.30584145e-01 6.03581620e-01\n",
      "  7.84394554e-01]\n",
      " [3.81175173e-01 9.97009702e-01 5.40217170e-01 1.23677479e-01\n",
      "  2.06073653e-01]\n",
      " [4.81037700e-01 2.20235035e-01 1.60480307e-01 1.56582050e-01\n",
      "  3.92690862e-01]\n",
      " [6.81535807e-01 6.11716434e-01 6.70841831e-04 1.22563416e-01\n",
      "  8.69756160e-01]\n",
      " [1.99574961e-01 6.39793567e-01 5.27311180e-01 3.74563864e-01\n",
      "  1.75605116e-01]\n",
      " [8.21352389e-01 2.70152208e-01 8.58420395e-01 6.87539419e-01\n",
      "  9.82503750e-01]\n",
      " [8.00457468e-01 3.89907862e-01 2.55645655e-01 6.16274029e-01\n",
      "  1.01501323e-01]\n",
      " [5.40102482e-01 4.54896520e-01 6.69297695e-01 7.49274869e-01\n",
      "  3.27391338e-01]\n",
      " [2.15067370e-01 8.48228699e-01 4.59483069e-01 2.62800316e-01\n",
      "  9.07962881e-01]\n",
      " [8.47114852e-01 8.64124393e-01 5.36945765e-01 5.54890536e-01\n",
      "  4.66914511e-01]] [2 2 1 3 1 0 1 0 4 1]\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from Metrics import ClosedSetAcc\n",
    "import numpy as np \n",
    "\n",
    "n_samples, C = 10, 5\n",
    "close_pred = np.random.rand(n_samples, C)\n",
    "close_labels = np.random.randint(low=0, high=C, size=(n_samples, ))\n",
    "print(close_pred, close_labels)\n",
    "\n",
    "acc = ClosedSetAcc(close_pred, close_labels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-set Accuracy at Given Ratio of Open-Set TPR performance \n",
    "> XCurve.Metrics.Acc_At_TPR(open_set_preds, open_set_labels, r=0.95)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the open-set accuracy at the given ratio of open-set TPR performance. For more details, please refer to the literature:\n",
    "> Open-Set Recognition: A Good Closed-Set Classifier is All You Need. Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman. ICLR, 2022.\n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- open_set_preds: Predicted open-set score (numpy array with shape (n_samples, )).\n",
    "- open_set_labels: True open-set label (numpy array with shape (n_samples,)). \n",
    "- r: The specific ratio of the TPR performance (float, 0.95 default).\n",
    "\n",
    "#### Return\n",
    "- Open-set Accuracy at Given Ratio of Open-Set TPR performance (int).\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18372168 0.59490336 0.97200531 0.58337086 0.36356957 0.21770302\n",
      " 0.69045764 0.6618183  0.77075417 0.42478645] [0 1 1 0 0 1 0 1 0 0]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from Metrics import Acc_At_TPR\n",
    "import numpy as np \n",
    "\n",
    "n_samples, C = 10, 5\n",
    "open_pred = np.random.rand(n_samples)\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(open_pred, open_labels)\n",
    "\n",
    "acc_at_95_tpr = Acc_At_TPR(open_pred, open_labels, 0.95)\n",
    "print(acc_at_95_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-set AUROC\n",
    "> XCurve.Metrics.AUROC(open_set_preds, open_set_labels)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the open-set AUROC, which is exactly the same as that for binary classification and novelty dectection. For more details, please refer to the literature:\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- open_set_preds: Predicted open-set score (numpy array with shape (n_samples, )).\n",
    "- open_set_labels: True open-set label (numpy array with shape (n_samples,)). \n",
    "\n",
    "#### Return\n",
    "- Open-set AUROC (int).\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81851032 0.37701845 0.68562654 0.79856968 0.45669198 0.98204199\n",
      " 0.94679142 0.52635628 0.76144028 0.22709308] [1 0 0 1 1 1 0 0 1 0]\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "from Metrics import AUROC\n",
    "import numpy as np \n",
    "\n",
    "n_samples, C = 10, 5\n",
    "open_pred = np.random.rand(n_samples)\n",
    "open_labels = np.random.randint(low=0, high=2, size=(n_samples, ))\n",
    "print(open_pred, open_labels)\n",
    "\n",
    "acc_at_95_tpr = AUROC(open_pred, open_labels)\n",
    "print(acc_at_95_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAUC\n",
    "> XCurve.Metrics.OpenAUC(open_set_pred_known, open_set_pred_unknown, close_set_pred_class, close_set_labels)\n",
    "\n",
    "### Brief Introductions\n",
    "Compute the OpenAUC score, which is denoted as:\n",
    "$$\n",
    "    \\text{OpenAUC}(h, r) = \\frac{1}{N_k N_u} \\sum_{i = 1}^{N_k} \\sum_{j = 1}^{N_u} \\mathbb{I}(h(x_i) = y_i) \\cdot \\mathbb{I}(r(x_j) > r(x_i)),\n",
    "$$\n",
    "where $h: \\mathcal{X} \\to \\mathcal{Y}_k$ represents the close-set classifier, $r: \\mathcal{X} \\to \\mathbb{R}$ denotes the open-set ranker; $(x_i, y_i)$ and $x_j$ are sampled from close-set and open-set, respectively.\n",
    "\n",
    "> OpenAUC: Towards AUC-Oriented Open-Set Recognition. Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao and Qingming Huang. NeurIPS, 2022. \n",
    "\n",
    "### Code Instructions\n",
    "\n",
    "#### Parameters\n",
    "- open_set_pred_known: Predicted open-set score of close-set samples (numpy array with shape (n_close_samples, )).\n",
    "- open_set_pred_unknown: Predicted open-set score of open-set samples (numpy array with shape (n_open_samples,)). \n",
    "- close_set_pred_class: Predicted close-set class of close-set samples (numpy array with shape (n_close_samples, )).\n",
    "- close_set_labels: True close-set class of close-set samples (numpy array with shape (n_close_samples, )).\n",
    "\n",
    "#### Return\n",
    "- OpenAUC (int).\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21843244 0.78820666 0.94065898 0.75685709 0.03735551 0.14710384\n",
      " 0.15949393 0.4660108  0.46239205 0.45981879] [0.95505156 0.39318565 0.73520436 0.79222155 0.91318986 0.83815728\n",
      " 0.21224934 0.10919423] [3 0 2 1 3 1 2 3 2 1] [1 1 4 2 3 4 0 1 0 1]\n",
      "0.16249999999999998\n"
     ]
    }
   ],
   "source": [
    "from Metrics import OpenAUC\n",
    "import numpy as np \n",
    "\n",
    "n_close_samples, C, n_open_samples = 10, 5, 8\n",
    "open_set_pred_known = np.random.rand(n_close_samples)\n",
    "open_set_pred_unknown = np.random.rand(n_open_samples)\n",
    "close_set_pred_class = np.random.randint(low=0, high=C, size=(n_close_samples, ))\n",
    "close_set_labels = np.random.randint(low=0, high=C, size=(n_close_samples, ))\n",
    "print(open_set_pred_known, open_set_pred_unknown, close_set_pred_class, close_set_labels)\n",
    "\n",
    "openauc = OpenAUC(open_set_pred_known, open_set_pred_unknown, close_set_pred_class, close_set_labels)\n",
    "print(openauc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
